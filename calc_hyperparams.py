import json
import numpy as np

def analyze_dataset_and_recommend(json_file_path):
    print(f"æ­£åœ¨è¯»å–æ ‡æ³¨æ–‡ä»¶: {json_file_path} ...")
    with open(json_file_path, 'r') as f:
        data = json.load(f)

    annotations = data.get('annotations', [])
    if not annotations:
        print("æœªæ‰¾åˆ°æ ‡æ³¨ä¿¡æ¯ï¼Œè¯·æ£€æŸ¥ JSON æ–‡ä»¶æ ¼å¼ï¼")
        return

    areas = []
    widths = []
    heights = []

    for ann in annotations:
        # COCO æ ¼å¼çš„ bbox æ˜¯ [x_min, y_min, width, height]
        bbox = ann.get('bbox', [])
        if len(bbox) == 4:
            w, h = bbox[2], bbox[3]
            if w > 0 and h > 0:
                widths.append(w)
                heights.append(h)
                areas.append(w * h)

    if not areas:
        print("æ²¡æœ‰æœ‰æ•ˆçš„è¾¹ç•Œæ¡†æ•°æ®ï¼")
        return

    areas = np.array(areas)
    widths = np.array(widths)
    heights = np.array(heights)

    # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
    avg_area = np.mean(areas)
    median_area = np.median(areas)
    avg_w = np.mean(widths)
    avg_h = np.mean(heights)
    avg_size = (avg_w + avg_h) / 2.0

    # ç»Ÿè®¡å°ç›®æ ‡æ¯”ä¾‹ (æŒ‰ç…§ COCO æ ‡å‡†ï¼Œarea < 32^2 = 1024 ç®—ä½œå°ç›®æ ‡)
    small_objects = np.sum(areas < 1024)
    small_ratio = small_objects / len(areas) * 100

    print("\n" + "="*40)
    print("ğŸ¯ æ•°æ®é›†åŸºç¡€ç»Ÿè®¡ä¿¡æ¯")
    print("="*40)
    print(f"æ€»ç›®æ ‡æ•°: {len(areas)}")
    print(f"å¹³å‡å®½åº¦: {avg_w:.2f} åƒç´ ")
    print(f"å¹³å‡é«˜åº¦: {avg_h:.2f} åƒç´ ")
    print(f"å¹³å‡ç»å¯¹å°ºå¯¸ ((w+h)/2): {avg_size:.2f} åƒç´ ")
    print(f"å¹³å‡é¢ç§¯: {avg_area:.2f} å¹³æ–¹åƒç´ ")
    print(f"ä¸­ä½æ•°é¢ç§¯: {median_area:.2f} å¹³æ–¹åƒç´ ")
    print(f"COCOæ ‡å‡†å°ç›®æ ‡æ¯”ä¾‹ (é¢ç§¯<1024): {small_ratio:.2f}%")

    print("\n" + "="*40)
    print("ğŸ’¡ è¶…å‚æ•°ä¿®æ”¹å»ºè®® (é’ˆå¯¹ uav_d_head.py)")
    print("="*40)

    # 1. æ¨è tau_scale
    # åœ¨ uav_d_head.py ä¸­: w_small = exp(-area / tau_scale)
    # å½“ area = tau_scale æ—¶ï¼Œw_small = exp(-1) â‰ˆ 0.36
    # ä¸ºäº†è®©æ¯”â€œå¹³å‡å¤§å°â€å°çš„ç›®æ ‡è·å¾—æ›´å¤§çš„ NWD æƒé‡ï¼Œtau_scale å»ºè®®è®¾ä¸ºå¹³å‡é¢ç§¯æˆ–ä¸­ä½æ•°é¢ç§¯
    recommended_tau = avg_area if avg_area < 2000 else median_area
    print(f"1. tau_scale (éš¾åº¦æ„ŸçŸ¥å°ºåº¦):")
    print(f"   å½“å‰ä»£ç ç¡¬ç¼–ç å€¼ : 900")
    print(f"   ğŸ’¡ æ¨èä¿®æ”¹ä¸º  : {int(recommended_tau)}")
    print(f"   è§£é‡Š: å½“ç›®æ ‡é¢ç§¯ç­‰äº {int(recommended_tau)} æ—¶ï¼ŒNWD ä¸ IoU çš„æ··åˆæƒé‡å°†å„å ä¸€åŠå·¦å³ã€‚")

    # 2. æ¨è nwd_constant
    # åŸè®ºæ–‡ä¸­ AI-TOD æ•°æ®é›†å¹³å‡å°ºå¯¸çº¦ä¸º 12.8 åƒç´ ï¼Œå› æ­¤ C=12.8
    # æˆ‘ä»¬åº”å½“æ ¹æ®ä½ è‡ªå·±çš„æ•°æ®é›†å¹³å‡å°ºå¯¸æ¥è®¾ç½®
    recommended_nwd_c = avg_size
    print(f"\n2. nwd_constant (NWD å¸¸æ•°åˆ†æ¯):")
    print(f"   å½“å‰ä»£ç ç¡¬ç¼–ç å€¼ : 12.8 (è¿™æ˜¯ AI-TOD çš„é»˜è®¤å€¼)")
    print(f"   ğŸ’¡ æ¨èä¿®æ”¹ä¸º  : {recommended_nwd_c:.1f}")
    print(f"   è§£é‡Š: è¯¥å¸¸æ•°ç”¨äºå°† Wasserstein è·ç¦»æ˜ å°„åˆ° 0~1 ä¹‹é—´ï¼Œå»ºè®®è®¾ç½®ä¸ºæ•°æ®é›†ç›®æ ‡çš„å¹³å‡ç»å¯¹å°ºå¯¸ã€‚")

    # 3. æ¨è inner_ratio
    print(f"\n3. inner_ratio (Inner-WIoU ç¼©æ”¾æ¯”):")
    if small_ratio > 50:
        rec_inner = 0.75
        explanation = "ä½ çš„æ•°æ®é›†ä¸­å°ç›®æ ‡éå¸¸å¤šï¼ˆ>50%ï¼‰ï¼Œæ›´å°çš„ ratio æœ‰åŠ©äºèšç„¦æ ¸å¿ƒç‰¹å¾ã€‚"
    else:
        rec_inner = 0.85
        explanation = "ä½ çš„æ•°æ®é›†ç›®æ ‡å°ºå¯¸è¾ƒä¸ºå‡è¡¡ï¼Œç»´æŒ 0.85 æ¯”è¾ƒåˆé€‚ã€‚"
    print(f"   å½“å‰ä»£ç é»˜è®¤å€¼ : 0.85")
    print(f"   ğŸ’¡ æ¨èä¿®æ”¹ä¸º  : {rec_inner}")
    print(f"   è§£é‡Š: {explanation}")

if __name__ == "__main__":
    # è¯·å°†è¿™é‡Œçš„è·¯å¾„æ›¿æ¢ä¸ºä½  base.py é‡Œçš„å®é™…è·¯å¾„
    JSON_PATH = 'D:\\UAV-OWD\\SOWOD_Merged_VOC\\COCO_JSONB\\instances_train_t1.json'
    analyze_dataset_and_recommend(JSON_PATH)